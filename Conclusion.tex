\section{Conclusions}
\label{sec:Conclusion}

This document summarizes a significant scaling study resulting from over 9
million core-hours of execution and analyzes the comparative performance of
multiple workflows for performing \vda on simulation results.  Most of
these workflows benefit from running in tandem with the simulation to
analyze its transient data before it is written to storage.  Based on this
analysis, we make the following conclusions.

\paragraph{\Intransit can provide a performance improvement over \insitu in
  some circumstances, but the window is narrower than we anticipated.}
\Intransit data analysis has an added overhead above embedded \insitu data
analysis involving transferring data between parallel jobs.  Given a data
analysis algorithm with perfect linear scalability, we suspect \intransit
workflows will always have an added cost, and our results support this.
With a data analysis algorithm that does not scale perfectly, possibly due
to communication overhead, it is theoretically possible for \intransit to
be faster by reducing the size of the data analysis job.  This is one of
the motivations for choosing a data analysis task that requires significant
communication.  In our results, we do find instances where \intransit is
faster, but by a smaller margin and for fewer configurations than we
initially anticipated.  So although \intransit has several other positive
features, we do not anticipate performance to be the main motivations for
using it.

\paragraph{The efficiency of \intransit relies on balancing the time spent
  in simulation and data analysis.}  The significant overhead cost, apart
from data transfer, in the \intransit workflow is the idle time spent in
the simulation waiting for the \vda service to become ready or the idle
time spent in the \vda service waiting for the simulation to send more
data.  This idle waiting time is minimized when the simulation and data
analysis spend the same amount of wall clock time between transfers.
Although not demonstrated in this work, it is possible to ``auto-balance''
the work between simulation and data analysis by, at every iteration of the
simulation, transfer data to the data analysis if and only if the data
analysis service is ready to accept more work.  The disadvantage of such an
approach is that the idle process time could be replaced with unnecessary
extra data analysis or less data analysis than necessary.  However, we
suspect that controlling the amount of \vda performed through job
allocation sizes fits well with users' rules of thumb about resource
allocation.

\paragraph{Memory overhead will be an important trade-off space.}
The baseline amount of memory added to the CTH job to perform \insitu
processing is roughly 100MB per core.  Considering that our embedded
\insitu library is a fully featured visualization toolkit containing over 2
million lines of code and algorithms developed over almost 2 decades, this
overhead is not unreasonable.  Nevertheless, this footprint can be
problematic for simulations already tight on memory.  Because of this,
efforts are already underway to improve our memory footprint by making
finer modules and being more selective on the available algorithms.  This,
of course, requires a compromise between the size of the library and the
algorithms that are dynamically available.  We also note that our algorithm
has the potential to generate sizable meshes of its own.  Thus, it may be
fruitful to pursue and support incremental algorithms where possible.

\paragraph{Initialization time matters.}  Our scaling efforts to date
focus on the scalability of the algorithms invoked during the run of a
simulation.  The initialization cost, a one-time penalty, has yet to be
seriously considered.  However, based on our HPCToolkit measurements,
initialization becomes a significant cost at high process counts.

\paragraph{Disk-based I/O is not dead\ldots{} yet.} Our initial assumption
was that it would not be feasible to output full results at a fine enough
temporal resolution from CTH to disk storage to perform our high fidelity
data analysis.  However, our control workflow shows that although the overall
time to write data to disk and then read back again incurs a large cost, it
is still realistic to do so.  Thus, users may still choose to incur the
extra overhead to use a traditional offline post-processing \vda workflow.

\paragraph{Better job scheduling is important.} One of the more complicated
parts of running an \intransit workflow is scheduling the simulation job
and service job to run in tandem.  Frankly, the capabilities of the
scheduler are inadequate for our needs.  We cannot start and stop jobs
independently and make reconnections dynamically.  Another experiment we
would like to do but is challenging to schedule is to allow simulation and
service to share nodes.  Since each node has 16 cores, perhaps we could get
better transfer performance by allocating one core per node for service and
the rest for simulation.  A similar scheduling scheme will be important to
take advantage of burst buffers in future architectures.

\fix{Timeline of work ahead.}
