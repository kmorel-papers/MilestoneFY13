\section*{Executive Summary}
\addcontentsline{toc}{section}{Executive Summary}

Milestone 4745, Data Co-Processing for Extreme Scale Analysis, is
successfully completed on time and demonstrated against the letter and
spirit of the stated Milestone.

Visualization and data analysis on extreme scale platforms presents
critical challenges in the management of data generated by simulations and
the interface between simulation and data analysis.  Computation speed will
continue to outpace storage bandwidth, and power management will become
much more of a workflow constraint on advanced architectures, so we
anticipate that science on extreme scale machines will require a range of
data analysis, filtering, and visualization workflows.  With these tools, the
analysts will determine 
the best computation profile for specific problems.  In particular, in 
addition to current
practices of post-processing and constrained writes, customers will need
\insitu and \intransit workflows that allow them flexible options for
getting results to persistent storage.

Milestone 4745 provides important study of the behavior of new workflows
for \vda.  In particular, we examine the performance of two proposed
workflows: an \insitu workflow in which \vda is coupled directly with a
simulation as a library, and an \intransit workflow in which \vda is a separate
service connected to the simulation via a network.  Each workflow has its
own characteristics, and our study details empirical evidence on their
respective performances.

As we look ahead to the extreme architectures planned by ASC, it is critical to
understand the strengths and weaknesses of these proposed analysis approaches,
and to design experiments that will help us understand how to enhance
scientific discovery on these new architectures.  Milestones such as this one
provide important foundations for evaluating our current technical approaches,
and for strategic development of analysis capabilities for future
architectures.

For this Milestone, we explored the performance characteristics of two proposed
technologies developed with significant contributions from Sandia National
Laboratories.  These technologies leverage existing investments in data
analysis, visualization and I/O research, and are part of a long term strategic
plan for addressing ASC analysis needs across a spectrum of scales, codes, and
science domains.  We employ two critical software technologies developed with
significant contributions from Sandia National Laboratories.  First, we use the
Catalyst library to provide \insitu \vda directly to a running simulation.
Second, we use the Nessie framework to establish an \intransit \vda service
connected to a running simulation.

Our primary motivation for this use case was to use a highly scalable code that
provided an example of real physics, so that we could determine performance
characteristics when applied against real data.  There is significant community
development in min-apps available in the community that scale well across large
architectures, but whose output is not representative of a real science code.
CTH is utilized as a test code for ASC platforms, and has been used in several
exploratory in-situ tests in the past.  Because of this, the team opted to use
CTH as a driver for this large scale experiment. 

Our empirical study comprises over 10 million core hours of running an
instrumented simulation and data analysis use case.  This use case, involving
the fragmentation analysis of an explosion simulated in the CTH shock
physics code, is designed in conjunction with a Sandia analysis customer as
an exemplar of scientific work.

In addition to demonstrating the scalability of our frameworks, our study
also provides insightful comparisons between the \insitu and \intransit
workflows and the trade-off point between them.  We also consider other
important parameters such as memory overhead, initialization time, and
scheduling.

This SAND report presents the full results of our milestone work and is
available to anyone.

